---
title: "Estimate cor â€” M rho"
author: "Yuxin Zou"
date: 2018-9-20
output: 
  workflowr::wflow_html:
    code_folding: hide
---

```{r}
library(mashr)
source('../code/generateDataV.R')
source('../code/summary.R')
```

We use EM algorithm to update $\rho$.

B is the $n\times R$ true value matrix. $\mathbf{z}$ is a length n vector.

## E step

$$
P(\hat{B},B|\rho, \pi) = \prod_{i=1}^{n} \left[N(\hat{b}_{i}; b_{i}, V)\sum_{p=0}^{P} \pi_{p} N(b_{i}; 0, \Sigma_{p})\right]
$$

$$
\begin{align*}
\mathbb{E}_{B|\hat{B}} \log P(\hat{B},B|\rho, \pi) &= \sum_{i=1}^{n} \mathbb{E}_{b_{i}|\hat{b}_{i}}\left[ \log N(\hat{b}_{i}; b_{i}, V) + \log \sum_{p=0}^{P} \pi_{p} N(b_{i}; 0, \Sigma_{p}) \right] \\
&= \sum_{i=1}^{n} \mathbb{E}_{b_{i}|\hat{b}_{i}}\log N(\hat{b}_{i}; b_{i}, V) + \sum_{i=1}^{n}\mathbb{E}_{b_{i}|\hat{b}_{i}}\log \sum_{p=0}^{P} \pi_{p} N(b_{i}; 0, \Sigma_{p})
\end{align*}
$$

$\rho$ depends on the first term only. Let $\mu_{i} = \mathbb{E}_{b_{i}|\hat{b}_{i}}(b_{i})$
$$
\begin{align*}
\log N(\hat{b}_{i}; b_{i}, V) &= -\frac{p}{2}\log 2\pi -\frac{1}{2}\log |V| - \frac{1}{2}(\hat{b}_{i}-b_{i})^{T}V^{-1}(\hat{b}_{i}-b_{i}) \\
&= -\frac{p}{2}\log 2\pi -\frac{1}{2}\log |V| - \frac{1}{2}\hat{b}_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}b_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}\hat{b}_{i}^{T}V^{-1}b_{i} -\frac{1}{2}b_{i}^{T}V^{-1}b_{i} \\
\mathbb{E}_{b_{i}|\hat{b}_{i}} \log N(\hat{b}_{i}; b_{i}, V) &= -\frac{p}{2}\log 2\pi -\frac{1}{2}\log |V| - \frac{1}{2}\hat{b}_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}\mu_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}\hat{b}_{i}^{T}V^{-1}\mu_{i} -\frac{1}{2}tr(V^{-1}\mathbb{E}_{b_{i}|\hat{b}_{i}}(b_{i}b_{i}^{T}))
\end{align*}
$$

V has a specific form:
$$
V = \left( \begin{matrix}1 & \rho \\ \rho & 1 \end{matrix} \right)
$$

$$
\begin{align*}
\mathbb{E}_{b_{i}|\hat{b}_{i}} \log N(\hat{b}_{i}; b_{i}, V) &= -\frac{p}{2}\log 2\pi -\frac{1}{2}\log |V| - \frac{1}{2}\hat{b}_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}\mu_{i}^{T}V^{-1}\hat{b}_{i} + \frac{1}{2}\hat{b}_{i}^{T}V^{-1}\mu_{i} -\frac{1}{2}tr(V^{-1}\mathbb{E}_{b_{i}|\hat{b}_{i}}(b_{i}b_{i}^{T})) \\
&= -\log 2\pi -\frac{1}{2}\log(1-\rho^2) - \frac{1}{2(1-\rho^2)}\left(\hat{b}_{i1}^2 + \hat{b}_{i2}^2 - 2\hat{b}_{i1}\hat{b}_{i2}\rho -2\hat{b}_{i1} \mu_{i1} -2\hat{b}_{i2} \mu_{i2} + 2\hat{b}_{i2}\mu_{i1}\rho + 2\hat{b}_{i1}\mu_{i2}\rho + \mathbb{E}(b_{i1}^2|\hat{b}_{i}) + \mathbb{E}(b_{i2}^2|\hat{b}_{i}) - 2\rho\mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right)
\end{align*}
$$

$$
f(\rho) = \sum_{i=1}^{n} -\log 2\pi -\frac{1}{2}\log(1-\rho^2) - \frac{1}{2(1-\rho^2)}\left(\hat{b}_{i1}^2 + \hat{b}_{i2}^2 - 2\hat{b}_{i1}\hat{b}_{i2}\rho -2\hat{b}_{i1} \mu_{i1} -2\hat{b}_{i2} \mu_{i2} + 2\hat{b}_{i2}\mu_{i1}\rho + 2\hat{b}_{i1}\mu_{i2}\rho + \mathbb{E}(b_{i1}^2|\hat{b}_{i}) + \mathbb{E}(b_{i2}^2|\hat{b}_{i}) - 2\rho\mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right)
$$

$$
\begin{align*}
f(\rho)' &= \sum_{i=1}^{n} \frac{\rho}{1-\rho^2} -\frac{\rho}{(1-\rho^2)^2}\left( \hat{b}_{i1}^2 + \hat{b}_{i2}^2 -2\hat{b}_{i1} \mu_{i1} -2\hat{b}_{i2} \mu_{i2} + \mathbb{E}(b_{i1}^2|\hat{b}_{i}) + \mathbb{E}(b_{i2}^2|\hat{b}_{i}) \right) -\frac{\rho^2+1}{(1-\rho^2)^2}\left( -\hat{b}_{i1}\hat{b}_{i2} + \hat{b}_{i1}\mu_{i2} +\hat{b}_{i2}\mu_{i1} - \mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right) = 0 \\
0 &= \rho(1-\rho^2)n - \rho \sum_{i=1}^{n} \left( \hat{b}_{i1}^2 + \hat{b}_{i2}^2 -2\hat{b}_{i1} \mu_{i1} -2\hat{b}_{i2} \mu_{i2} + \mathbb{E}(b_{i1}^2|\hat{b}_{i}) + \mathbb{E}(b_{i2}^2|\hat{b}_{i}) \right) - (\rho^2 + 1) \sum_{i=1}^{n} \left( -\hat{b}_{i1}\hat{b}_{i2} + \hat{b}_{i1}\mu_{i2} +\hat{b}_{i2}\mu_{i1} - \mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right) \\
0 &=-n\rho^{3} - \rho^2 \sum_{i=1}^{n} \left( -\hat{b}_{i1}\hat{b}_{i2} + \hat{b}_{i1}\mu_{i2} +\hat{b}_{i2}\mu_{i1} - \mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right) - \rho \sum_{i=1}^{n} \left( \hat{b}_{i1}^2 + \hat{b}_{i2}^2 -2\hat{b}_{i1} \mu_{i1} -2\hat{b}_{i2} \mu_{i2} + \mathbb{E}(b_{i1}^2|\hat{b}_{i}) + \mathbb{E}(b_{i2}^2|\hat{b}_{i}) -1\right) - \sum_{i=1}^{n} \left( -\hat{b}_{i1}\hat{b}_{i2} + \hat{b}_{i1}\mu_{i2} +\hat{b}_{i2}\mu_{i1} - \mathbb{E}(b_{i1}b_{i2}|\hat{b}_{i}) \right)
\end{align*}
$$
The polynomial has either 1 or 3 real roots in (-1, 1).

It is hard to estimate $\boldsymbol{\pi}$ from the second term.

## Solution

Given $\rho$, we estimate $\boldsymbol{\pi}$ by max loglikelihood (convex problem)

Algorithm: 
```{r, eval=FALSE, tindy = FALSE}
Input: X, Ulist, init_rho
Given rho, estimate pi by max loglikelihood (convex problem)
Compute loglikelihood
delta = 1
while delta > tol
  M step: update rho
  Given rho, estimate pi by max loglikelihood (convex problem)
  Compute loglikelihood
  Update delta
```

```{r}
#' @param rho the off diagonal element of V, 2 by 2 correlation matrix
#' @param Ulist a list of covariance matrices, U_{k}
get_sigma <- function(rho, Ulist){
  V <- matrix(c(1,rho,rho,1), 2,2)
  lapply(Ulist, function(U) U + V)
}

penalty <- function(prior, pi_s){
  subset <- (prior != 1.0)
  sum((prior-1)[subset]*log(pi_s[subset]))
}

#' @title compute log likelihood
#' @param L log likelihoods,
#' where the (i,k)th entry is the log probability of observation i
#' given it came from component k of g
#' @param p the vector of mixture proportions
#' @param prior the weight for the penalty
compute.log.lik <- function(lL, p, prior){
  p = normalize(pmax(0,p))
  temp = log(exp(lL$loglik_matrix) %*% p)+lL$lfactors
  return(sum(temp) + penalty(prior, p))
  # return(sum(temp))
}

normalize <- function(x){
  x/sum(x)
}
```

```{r}
mixture.M.rho.times <- function(X, Ulist, init_rho=0, tol=1e-5, prior = c('nullbiased', 'uniform')){
  times = length(init_rho)
  result = list()
  loglik = c()
  rho = c()
  time.t = c()
  converge.status = c()
  for(i in 1:times){
    out.time = system.time(result[[i]] <- mixture.M.rho(X, Ulist,
                                                      init_rho=init_rho[i],
                                                      prior=prior,
                                                      tol=tol))
    time.t = c(time.t, out.time['elapsed'])
    loglik = c(loglik, tail(result[[i]]$loglik, 1))
    rho = c(rho, result[[i]]$rho)
  }
  if(abs(max(loglik) - min(loglik)) < 1e-4){
    status = 'global'
  }else{
    status = 'local'
  }
  ind = which.max(loglik)
  return(list(result = result[[ind]], status = status, loglik = loglik, rho=rho, time = time.t))
}

mixture.M.rho <- function(X, Ulist, init_rho=0, tol=1e-5, prior = c('nullbiased', 'uniform')) {

  prior <- match.arg(prior)

  m.model = fit_mash(X, Ulist, rho = init_rho, prior=prior)
  pi_s = get_estimated_pi(m.model, dimension = 'all')
  prior.v <- mashr:::set_prior(length(pi_s), prior)

  # compute loglikelihood
  loglik <- c()
  loglik <- c(loglik, get_loglik(m.model)+penalty(prior.v, pi_s))
  delta.ll <- 1
  niter <- 0
  rho = init_rho

  while(delta.ll > tol){
    # max_rho
    rho <- E_rho(X, m.model)

    m.model = fit_mash(X, Ulist, rho, prior=prior)

    pi_s = get_estimated_pi(m.model, dimension = 'all')

    loglik <- c(loglik, get_loglik(m.model)+penalty(prior.v, pi_s))
    # Update delta
    delta.ll <- loglik[length(loglik)] - loglik[length(loglik)-1]
    niter <- niter + 1
  }

  return(list(pihat = normalize(pi_s), rho = rho, loglik=loglik))
}

E_rho <- function(X, m.model){
  n = nrow(X)
  post.m = m.model$result$PosteriorMean
  post.sec = plyr::laply(1:n, function(i) m.model$result$PosteriorCov[,,i] + tcrossprod(post.m[i,])) # nx2x2 array

  temp2 = -sum(X[,1]*X[,2]) + sum(X[,1]*post.m[,2]) + sum(X[,2]*post.m[,1]) - sum(post.sec[,1,2])
  temp1 = sum(X[,1]^2 + X[,2]^2) - 2*sum(X[,1]*post.m[,1]) - 2*sum(X[,2]*post.m[,2]) + sum(post.sec[,1,1] + post.sec[,2,2])

  rts = polyroot(c(temp2, temp1-n, temp2, n))

  # check complex number
  is.real = abs(Im(rts))<1e-12
  if(sum(is.real) == 1){
    return(Re(rts[is.real]))
  }else{
    print('3 real roots')
    return(Re(rts))
  }
}

fit_mash <- function(X, Ulist, rho, prior=c('nullbiased', 'uniform')){
  m.data = mashr::mash_set_data(Bhat=X, Shat=1, V = matrix(c(1, rho, rho, 1), 2, 2))
  m.model = mashr::mash(m.data, Ulist, prior=prior, verbose = FALSE, outputlevel = 3)
  return(m.model)
}
```

## Data

```{r}
set.seed(1)
n = 4000; p = 2
Sigma = matrix(c(1,0.5,0.5,1),p,p)
U0 = matrix(0,2,2)
U1 = U0; U1[1,1] = 1
U2 = U0; U2[2,2] = 1
U3 = matrix(1,2,2)
Utrue = list(U0=U0, U1=U1, U2=U2, U3=U3)
data = generate_data(n, p, Sigma, Utrue)
```

```{r}
m.data = mash_set_data(data$Bhat, data$Shat)
U.c = cov_canonical(m.data)

result.mrho <- mixture.M.rho.times(m.data$Bhat, U.c)
```

The estimated $\rho$ is `r result.mrho$rho`. The running time is `r result.mrho$time` seconds.

```{r}
m.data.mrho = mash_set_data(data$Bhat, data$Shat, V = matrix(c(1,result.mrho$rho,result.mrho$rho,1),2,2))
U.c.mrho = cov_canonical(m.data.mrho)
m.mrho = mash(m.data.mrho, U.c, verbose= FALSE)
null.ind = which(apply(data$B,1,sum) == 0)
```

The log likelihood is `r formatC(get_loglik(m.mrho), digits=7)`. There are `r length(get_significant_results(m.mrho))` significant samples, `r sum(get_significant_results(m.mrho) %in% null.ind)` false positives. The RRMSE is `r RRMSE(data$B, data$Bhat, list(m.mrho))`.

