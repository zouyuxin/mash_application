---
title: "Estimate cor -- max"
author: "Yuxin Zou"
date: 2018-07-24
output: 
  html_document:
    code_folding: hide
---

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

```{r}
library(mashr)
source('../code/generateDataV.R')
source('../code/estimate_cor.R')
source('../code/summary.R')
library(kableExtra)
library(knitr)
```

$$
\left(\begin{matrix} \hat{x} \\ \hat{y} \end{matrix} \right) | \left(\begin{matrix} x \\ y \end{matrix} \right) \sim N(\left(\begin{matrix} \hat{x} \\ \hat{y} \end{matrix} \right) ; \left(\begin{matrix} x \\ y \end{matrix} \right), \left( \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right))
$$
$$
\left(\begin{matrix} x \\ y \end{matrix} \right) \sim \sum_{k=0}^{K} \pi_{k} N( \left(\begin{matrix} x \\ y \end{matrix} \right); 0, U_{k} )
$$
$\Rightarrow$
$$
\left(\begin{matrix} \hat{x} \\ \hat{y} \end{matrix} \right) \sim \sum_{k=0}^{K} \pi_{k} N( \left(\begin{matrix} \hat{x} \\ \hat{y} \end{matrix} \right); 0, \left( \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right) + U_{k} )
$$
$$
\Sigma_{k} = \left( \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right) + U_{k} = \left( \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right) + \left( \begin{matrix} u_{k11} & u_{k12} \\ u_{k21} & u_{k22} \end{matrix} \right) = \left( \begin{matrix} 1+u_{k11} & \rho+u_{k12} \\ \rho+u_{k21} & 1+u_{k22} \end{matrix} \right)
$$
Let $\sigma_{k11} = \sqrt{1+u_{k11}}$, $\sigma_{k22} = \sqrt{1+u_{k22}}$, $\phi_{k}=\frac{\rho+u_{k12}}{\sigma_{k11}\sigma_{k22}}$

# MLE

The loglikelihood is (with penalty)
$$
l(\rho, \pi) = \sum_{i=1}^{n} \log \sum_{k=0}^{K} \pi_{k}N(x_{i}; 0, \Sigma_{k}) + \sum_{k=0}^{K} (\lambda_{k}-1) \log \pi_{k}
$$

The penalty on $\pi$ encourages over-estimation of $\pi_{0}$, $\lambda_{k}\geq 1$.

$$
l(\rho, \pi) = \sum_{i=1}^{n} \log \sum_{k=0}^{K} \pi_{k}\frac{1}{2\pi\sigma_{k11}\sigma_{k22}\sqrt{1-\phi_{k}^2}} \exp\left( -\frac{1}{2(1-\phi_{k}^2)}\left[ \frac{x_{i}^2}{\sigma_{k11}^2} + \frac{y_{i}^2}{\sigma_{k22}^2} - \frac{2\phi_{k}x_{i}y_{i}}{\sigma_{k11}\sigma_{k22}}\right]  \right) + \sum_{k=0}^{K} (\lambda_{k}-1) \log \pi_{k}
$$

**Note:** This probelm is convex with respect to $\pi$. In terms of $\rho$, the covenxity depends on the data.

Algorithm:
```{r, eval=FALSE, tidy=FALSE, highlight=FALSE}
Input: X, init_pi, init_rho, Ulist
Compute loglikelihood
delta = 1
while delta > tol
  Given pi, estimate rho by max loglikelihood (optim function)
  Given rho, estimate pi by max loglikelihood (convex problem)
  Compute loglikelihood
  Update delta
```

# Data

$$
\hat{\beta}|\beta \sim N_{2}(\hat{\beta}; \beta, \left(\begin{matrix} 1 & 0.5 \\
                                          0.5 & 1 \end{matrix}\right))
$$

$$
\beta \sim \frac{1}{4}\delta_{0} + \frac{1}{4}N_{2}(0, \left(\begin{matrix} 1 & 0 \\
                                          0 & 0 \end{matrix}\right)) + \frac{1}{4}N_{2}(0, \left(\begin{matrix} 0 & 0 \\
                                          0 & 1 \end{matrix}\right)) + \frac{1}{4}N_{2}(0, \left(\begin{matrix} 1 & 1 \\
                                          1 & 1 \end{matrix}\right))
$$

n = 4000

```{r}
set.seed(1)
n = 4000; p = 2
Sigma = matrix(c(1,0.5,0.5,1),p,p)
U0 = matrix(0,2,2)
U1 = U0; U1[1,1] = 1
U2 = U0; U2[2,2] = 1
U3 = matrix(1,2,2)
Utrue = list(U0=U0, U1=U1, U2=U2, U3=U3)
data = generate_data(n, p, Sigma, Utrue)
```

```{r}
m.data = mash_set_data(data$Bhat, data$Shat)
U.c = cov_canonical(m.data)
grid = mashr:::autoselect_grid(m.data, sqrt(2))
Ulist = mashr:::normalize_Ulist(U.c)
xUlist = mashr:::expand_cov(Ulist,grid,usepointmass =  TRUE)
result <- optimize_pi_rho_times(data$Bhat, xUlist, init_rho = c(-0.7,0,0.7))
plot(result[[1]]$loglik)
```
The estimated $\rho$ is `r result[[1]]$rho`.

```{r}
m.data.mle = mash_set_data(data$Bhat, data$Shat, V = matrix(c(1,result[[1]]$rho,result[[1]]$rho,1),2,2))
U.c = cov_canonical(m.data.mle)
m.mle = mash(m.data.mle, U.c, verbose= FALSE)
null.ind = which(apply(data$B,1,sum) == 0)
```

The log likelihood is `r round(get_loglik(m.mle),2)`. There are `r length(get_significant_results(m.mle))` significant samples, `r sum(get_significant_results(m.mle) %in% null.ind)` false positives. The RRMSE is `r RRMSE(data$B, data$Bhat, list(m.mle))`.

The estimated `pi` is
```{r}
barplot(get_estimated_pi(m.mle), las=2, cex.names = 0.7, main='MLE', ylim=c(0,0.8))
```


The ROC curve:
```{r}
m.data.correct = mash_set_data(data$Bhat, data$Shat, V=Sigma)
m.correct = mash(m.data.correct, U.c, verbose = FALSE)
m.correct.seq = ROC.table(data$B, m.correct)
m.mle.seq = ROC.table(data$B, m.mle)
```

```{r echo=FALSE, fig.align = "center"}
{plot(m.correct.seq[,'FPR'], m.correct.seq[,'TPR'],type='l',xlab = 'FPR', ylab='TPR', main='True Positive vs False Positive', cex=1.5, lwd = 1.5)
lines(m.mle.seq[,'FPR'], m.mle.seq[,'TPR'], col='red', lwd = 1.5)
legend('bottomright', c('True','MLE'),col=c('black','red'),lty=c(1,1), lwd=c(1.5,1.5))}
```

# Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
